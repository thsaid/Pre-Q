---
title: "PALS"
author: "Tamer Said"
date: "01/11/2020"
output: html_document
---

Summary of the PALS Questions and their factors
Mastery Goals (MG): Q01:Q05
Performance Approach Goals (PAPG) Q06:Q10
Performance Avoidance Goals (PAVG) Q11:Q14
Self-efficacy (SE) Q15:Q19


#Intro to SALES survey

Q7-Q14	SE (8 questions)
Q15-Q22	TV (8 questions)
Q23-Q32	MG (8 questions), Question 30 is missing. 
Notes: Questions 26 and 31 are repeated
Questions 24 and 29 are repeated


```{r}
PreQ <- read.csv ("PreQ_filtered.csv")
```

Loading #libraries 

```{r}
library(tidyverse) # For data wrangling
library(ggplot2) # for graphing / figures - mostly histograms/ppplots here
library(pastecs) # Needed to run normality tests
library(car) # Needed to run Levene test
library(lsr) # Navarro package for running psychology tests
library(psych) # for key psychology stats
library(effects) # Effects package, needed for the estimated means, includes lower/upper 95% conf limits
options(scipen=99) # This is to indicate how many digits after the decimal, this one is for 2 digits, but can be changed
```

Selecting  the columns in PALS

```{r}
PALS <- select(.data = PreQ,ID,Gender, Certificate, Q27_1:Q27_19)
```

#Renaming question numbers

```{r}
PALS <- rename(PALS, Q01 = Q27_1, Q02 = Q27_2, Q03= Q27_3, Q04= Q27_4, Q05 = Q27_5, Q06 = Q27_6, Q07= Q27_7, Q08= Q27_8, Q09= Q27_9, Q10=Q27_10, Q11=Q27_11, Q12=Q27_12, Q13=Q27_13, Q14 = Q27_14, Q15= Q27_15, Q16=Q27_16, Q17=Q27_17, Q18=Q27_18, Q19=Q27_19)

```

Saving PALS into csv file

```{r}
write.csv(PALS, "PALS-R.csv")
```

```{r}
PALS <- read.csv("PALS-mod.csv")
PALS <- select(.data = PALS, ID:Q19)
```
I saved an earlier version of this file, and loaded it here again.

#Missing Data

```{r}
is.na(PALS)
```

It appears that 3 students have not attempted the survey at all. Those students will be removed since their data cannot be imputed.Those are rows: 13,75,451

```{r}
PALS <- PALS[-c(13,75,451), ]
```

```{r}
summary(PALS)
```

```{r}
install.packages("mice")
library(mice)
```

```{r}
md.pattern(PALS)
```
The table above shows us that we have 533 complete cases. The rest are missing some answers as indicated in the table above. 
```{r}
install.packages("VIM")
library(VIM)
mice_plot <- aggr(PALS, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(PALS), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"))
```

Based on the plot, it appears that 97% of the data is not missing. then the rest of the questions are missing less than 1% of the response (0.37%). 

```{r}
imputed_Data <- mice(PALS, m=5, maxit = 5, method = 'pmm', seed = 500) #m  – Refers to 5 imputed data sets (default is 5), maxit –  Refers to no. of iterations taken to impute missing values, deafult is 5. Seed: An integer that is used as argument by the set.seed() for offsetting the random number generator. Default is to leave the random number generator alone. method – Refers to method used in imputation. we used predictive mean matching 'pmm'. 
```


```{r}
imputed_Data$imp$Q04 #these command shows the imputed data for Q04 for all the versions (datasets)
```
I can choose any of the complete data using the following function: 

```{r}
completeData <- complete(imputed_Data,2) #in this case, I am selecting one version of the complete data, (no.2), there is a total of 5 versions in my case 
```

```{r}
summary(completeData) #as we can see, all the missing values have been replaced
```
Now, I will replace the missing values withe dataset no. 3 from Mice

```{r}
PALS <- complete(imputed_Data,3)
```


Now I am going to arrange the PALS responses in long format 

```{r}
PALS_long <- pivot_longer(data = PALS,cols =  Q01:Q19,names_to = "Question",values_to = "Response"  )
```
It appears that the long data format is affecting my analysis, since each case is counted 15 times. 


Joining the two tables to calculate scores

```{r}
PALS_joined <- inner_join (x = PALS_long,y= PALS_scoring, by ="Response", group_by(ID))
```

Calculating scores
```{r}
PALS_scores <- PALS_joined %>% 
             group_by(ID)
             
```


```{r}
PALS_scores <- PALS_joined %>% 
  group_by(ID) %>%
    summarise (PALS_Score = sum(score))
```

Saving ID numbers and PALS scores

```{r}
write.csv(PALS_scores, "PALS_scores.csv")
```

#Initial analysis before CFA.

I now need to group students' responses by question to do my analysis.

```{r}
PALS_Q <- PALS_joined %>% 
  group_by(Question)
```


I need to create a total coloumn for the score of each participant in the PALS questions
First,I will create total variable to sum numeric values across rows:

```{r}
total = rowSums(Filter(is.numeric, PALS), na.rm = TRUE)
```

#Calcualting total score for each participant. 
Now, I will create a sum coloumn for the total of student's responses in the questions


```{r}
PALS <- mutate(.data = PALS,sum = total)
```


```{r}
write.csv(PALS, "PALS.csv")
```

```{r}
PALS_Sum <- hist(PALS$sum) 
PALS_Sum + stat_function(fun = dnorm, args = list(mean = mean(PALS$sum,
na.rm = TRUE), colour = "black", size = 1)
```

#Parametric assumptions

This can be done via histograms on the sum of the scores. I am not sure of shall I do this on the responses for each question?

```{r}
PALS_hist <- ggplot(PALS, aes(sum)) + 
geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
labs(x = "score", y = "")
```
Adding normal curve

```{r}
PALS_hist + stat_function(fun = dnorm, args = list(mean = mean(PALS$sum,
na.rm = TRUE), sd = sd(PALS_Q$score, na.rm = TRUE)), colour = "black", size = 1)
```

Drawing Q-Plot to check the distribution of my scores compared to normal distribution

```{r}
qqplot.scores <- qplot(sample = PALS$sum, stat="qq")
```
There was an error with stat function and it says it is deprecated, I am not sure whether the resulting qplot is meanigful or not. 
 
Descrbing the scores per student

```{r}
summary(PALS)
```


```{r}
describe(PALS$sum)
```

Shapiro test:
```{r}
round(stat.desc(PALS$sum, basic = FALSE, norm = TRUE), digits= 3)
```
Saving the results into a file using sink command

```{r}
sink("PALS_Shapiro.docx")
print(round(stat.desc(PALS$sum, basic = FALSE, norm = TRUE), digits= 3))
sink()
```
The file is saved as txt file

#Notes on normality of scores:
skewness: -ve score indicate a build up of high scores
kurtosis: -0.389

-if skew.2SE (0.96) or kurt.2SE(-0.930) are greater than 1 (ignoring the plus or minus sign) then you have
significant skew/kurtosis (at p < .05)

It looks that my data is mostly normal.. the median and mean are very close. there is some degree ofkurtosis -0.4 and skewness of 0.2. 


```{r}
shapiro.test(PALS$sum)
```

#Shapiro test:
The shapiro scores is W = 0.95313, p-value = 0.000000000003767, which shows a significant non-normality. However, since the sample size is large, one shall be careful when interpreting these results since they get distorted at large sample sizes (Field, 2002)

Shapiro test across genders

```{r}
by(PALS$sum, PALS$Gender, shapiro.test)
```

Descriptive stats split by gender

```{r}
describeBy(PALS$sum, group = PALS$Gender)
```

```{r}
describeBy(PALS$sum, group = "Question")
```

Loading ggpubr library for graphs, then creating QQ plot to compare the deviation of my sample from the theoretical nornmal distribution

```{r}
library(ggpubr)
ggqqplot(PALS$sum)
```
The graph showed a normal distribution  when compared to the theoritical, it appears that there are some outliers, probably those who did not fill the PALS. 

Tried to do a dot chart for each question to compare their scores. 

```{r}
ggdotchart(PALS_Q$score, x = "question", y = "density",
           color = "Question",                                # Color by groups
           palette = c("#00AFBB", "#E7B800", "#FC4E07"), # Custom color palette
           sorting = "ascending",                        # Sort value in descending order
           add = "segments",                             # Add segments from y = 0 to dots
           ggtheme = theme_pubr()                        # ggplot2 theme
           )
```
Code did not work

```{r}
ggbarplot(PALS_Q, x = "Question", y = "Response",
          fill = "Question", sort.by.groups = TRUE)
```
The code worked, but the color of the bars did not show up.

Trying a dotchart
```{r}
ggdotchart(PALS_Q, x = "Question", y = "score",
           color = "Question",                                 # Custom color palette
           sorting = "descending",                       # Sort value in descending order
                                                   # Rotate vertically
           group = "Question",                                # Order by groups
           dot.size = 6,                                 # Large dot size
           label = round(PALS_Q$score),                        # Add mpg values as dot labels
           font.label = list(color = "white", size = 9,
                             vjust = 0.5),               # Adjust label parameters
           ggtheme = theme_pubr()                        # ggplot2 theme
)

```


#Homeogenity of variance

Using leven's test:
leveneTest(outcome variable, group, center = median/mean)

Testing the variance across gender.

```{r}
leveneTest(PALS$sum, PALS$Gender)
```

Since pr is 0.34, it means that the variance across gender is not significant
Next,I need to run the test for each question, to check the variance across questions.  

```{r}
leveneTest(PALS_Q$score, PALS_Q$Question)
```
F(1,10254)=45.3, p= 0.02, this result shows a significant variance across the questions. This result should be interpreted with caution, since large sample sizes may make Levene's test signficiant, even if the variance is not great.
Another test that is used is the variance ration (not sure though if need)

```{r}

```

#Correlations

```{r}
install.packages("Hmisc"); install.packages("ggm");
install.packages("ggplot2"); install.packages("polycor")
```

```{r}
library(boot); library(ggm); library(ggplot2); library(Hmisc);
library(polycor)
```
Running correlations for the whole data frame. This command below excludes the missing values and those that are not numeric.


```{r}
cor(PALS[sapply(PALS, is.numeric)], use='pairwise')
```


Saving the correlation matrix to data

```{r}
PALScorr <- cor(PALS[sapply(PALS, is.numeric)], use='pairwise')
```


Running Barlette test on the correlation matrix in prep for FA. The p value shall be less than 0.5.

A significant test tells us that the R-matrix is not an identity matrix(identity matrix is the case when variables are NOT related to each other and corr is zero); Therefore, if it is significant then it means that the correlations between variables are (overall) significantly different from zero, if Bartlett’s test is significant then it is good news (Field, 2009)


```{r}
cortest.bartlett(PALScorr, n=546)
```

Running KMO test for the degree of common variance (ideally, a score of 0.5 or higher is good). values close to 1 indicates that patterns of correlations are relatively compact and so factor analysis should yield distinct and reliable factors.

```{r}
KMO(PALScorr)
```

Finding the #determinant

```{r}
det(PALScorr)
```

```{r}
cor(PALS$sum, PALS$Gender, use = "complete.obs", method = "pearson")
```

It did not work, since Gender is a class variable and it needs to be numeric for correlation to take place. 

Some correlation commands do not work on data frames, so would have to convert my df into matrix.

```{r}
PALSMatrix <- as.matrix(PALS)
```
Runnning general correlation on the whole df, and saving it into PALScorr

```{r}
PALScorr <- cor(PALS[sapply(PALS, is.numeric)], use='pairwise')
```


#EFA: I need to start with EFA first and some preparatory analysis

Installing packages and loading libraries
```{r}
install.packages("corpcor"); install.packages("GPArotation"); install.
packages("psych")

library(corpcor); library(GPArotation); library(psych)
```

#PCA:I have set the factors to 19, which is the number of the items. This is just an exploratory step

```{r}
pc19 <- principal(PALScorr, nfactors = 19, rotate = "none")
```

```{r}
pc19
```
SS loading: refer to sum of squared loadings
```{r}
pc19$values
```

This command allows to view the eigenvalues of each factor. 

It appears that factor one 6.29 units of the variance in the model ,which = 6.29/19 = 0.33 or 33% of the variance 

Screeplot to determine the eigenvalues

```{r}
plot(pc19$values, type = "b")
```
Based on the scree plot, it may be feasible to test the 4 components model, since the inflection point is in the 4th component. This also agrees with my theoritical model.

```{r}
pc04 <- principal(PALScorr, nfactors = 4, rotate = "none")
pc04
```
#h2 (commonalities) show the amount of variance in each variable that can
be explained by the retained factors is represented by the communalities after extraction

we can also obtain the factor model using this function

```{r}
factor.model(pc04$loadings)
```

To compare our factor loadings with the orginial corr. matrix, we use the following:

```{r}
factor.residuals(PALScorr, pc04$loadings)
```
The above output  contains the differences between the observed correlation coefficients and the ones predicted from the model. For a good model these values will all be small.

Saving the residuals in a separate object

```{r}
residuals <- factor.residuals(PALScorr, pc04$loadings)
```

To extract the upper triangle of the residuals and save it as a matrix

```{r}
residuals<-as.matrix(residuals[upper.tri(residuals)])
```
i will save the large residuals (> 0.05) in a separate object

```{r}
large.resid<-abs(residuals) > 0.05
```

The residuals that are greater than 0.05 will be labelled as True, others will be False.

```{r}
sum(large.resid)
```
The sum is 43, to see the proportion of these,
```{r}
sum(large.resid)/nrow(residuals)
```

This means the 22% of our data have residuals larger than 0.05. Which is a good indicator

To view the residuals graphically

```{r}
hist(residuals)
```
As we can see, most of the residuals fall below 0.05

After doing EFA, let's move to CFA

Loading libraries

```{r}
library(tidyverse) # For data wrangling
library(lavaan) # For CFA/MI/SEM
library(semPlot) # For CFA/MI/SEM
library (semTools) # For CFA/MI/SEM
library(OpenMx) # For SEM
library(pastecs) # Needed to run normatlity tests
library(car) # Needed to run Levene test
library(lsr) # Navarro package for running psychology tests
library(psych) # for key psychology stats
library(effects) # Effects package, needed for the estimated means, includses lower/upper 95% conf limits
options(scipen=99) # This is to indicate how many digits after the decimal, this one is for 2 digits, but can be changed
```

# CFA and 1-defining the model

Mastery Goals (MG): Q01:Q05
Performance Approach Goals (PAPG) Q06:Q10
Performance Avoidance Goals (PAVG) Q11:Q14
Self-efficacy (SE) Q15:Q19
Let's start be defining the measurement model

```{r}
latents.model <- 'MasteryGoals =~ Q02 + Q01  + Q03 + Q04 + Q05
PerfAppGoals =~  Q09 + Q06 + Q07 + Q08 + Q10
PerfAvGoals =~ Q14 + Q11 + Q12 + Q13 
SelfEfficacy =~ Q16 + Q15 + Q17 + Q18 + Q19'
```

Run CFA model


```{r}
PALSfit <- cfa(latents.model, data = PALS, se = "robust", test = "Satorra-Bentler") #robust ML fit
summary(PALSfit) #to view summary of the fit
partable(PALSfit) # to have the summary of parameters converted to a dataframe
vartable(PALSfit) # to have the summary of variables converted to a dataframe
parameterestimates(PALSfit) #parameterEstimates() includes the estimates, standard errors, z value, p value, and 95% CIs for all model parameters.
summary(PALSfit,fit.measures = TRUE)
```
To see a summary of the tests and esimtates of the model

```{r}
summary(PALSfit,fit.measures = TRUE)
```
 To import the output into a file
 
```{r}
sink("PALS_fit.doc")
print(summary(PALSfit,fit.measures = TRUE))
sink()
```
 

 
#Notes on the fit measures
CFI and TLI needs to be high, as they compare your model to a completely free model. Comparative Fit Index (CFI)                    0.920
Tucker-Lewis Index (TLI)                       0.907 -
RMSEA: needs to be low as possible (< 0.06 is prefered)

plotting a sem diagram

```{r}
semPaths(PALSfit, title= FALSE, curvePivot= TRUE)
```
Another way of viewing the plot

```{r}
semPaths(PALSfit, "std", edge.label.cex=0.5, 
         curvePivot = TRUE, exoVar = FALSE)
```

Note: the thicker the lines, the higher the loadings. 

Will now try Michelle's script and save it as fit2

```{r}
PALSfit2 <- cfa(latents.model, data = PALS, se = "robust", test = "Satorra-Bentler") 
summary(PALSfit, rsq = TRUE,fit.measures = TRUE)
fitmeasures(PALSfit)
varTable(PALSfit)
parameterEstimates(PALSfit)
```

Exactly the same outcome..it seems that there is no differnece between the two methods. 

More plots (from Michelle)

```{r}
 #the model plot below shows up as a black & white plot with nonstandardised parameter estimates
semPaths(PALSfit, what = "path", whatLabels = "par", style = "lisrel", layout = "tree", 
         intercepts = TRUE, residuals = TRUE, thresholds = TRUE, 
         nCharNodes = 8, nCharEdges = 3, 
         sizeMan = 9, sizeMan2 = 4, sizeLat = 9, sizeLat2 = 7, sizeInt = 6, sizeInt2 = 4)
# the model plot below shows up as a black & white plot with standardised parameter estimates
semPaths(PALSfit, what = "path", whatLabels = "std", style = "lisrel", layout = "tree", 
         intercepts = TRUE, residuals = TRUE, thresholds = TRUE, 
         nCharNodes = 8, nCharEdges = 3, 
         sizeMan = 9, sizeMan2 = 4, sizeLat = 9, sizeLat2 = 7, sizeInt = 6, sizeInt2 = 4)
```
# Standardized vs. non-standardized paramets: 

The above plot we can see 2 ways of measuring the scale of the latents, in the non-standardized, the first factor is set to 1.0, which is also called the marker variable. This is used as a reference for the ohter variables in the model. The other way to measure the scale of the latents, is to standardize the scores (using Z-scores)

#Trying out 3 factor model instead

A high correlation was found between Pefromance app. and performance avoidance. Michelle suggested to try combining them together, and see if the model's fit improved by doing so. 


# CFA and defining the 2nd model

Mastery Goals (MG): Q01:Q05
Performance Goals (PG) Q06:Q14
Self-efficacy (SE) Q15:Q19
Let's start be defining the measurement model

```{r}
latents.model2 <- 'MasteryGoals =~ + Q02 + Q03 + Q01  + Q04 + Q05
PerfGoals =~ Q09 + Q06 + Q07 + Q08  + Q10 + Q11 + Q12 + Q13 + Q14
SelfEfficacy =~ Q16 + Q15 + Q17 + Q18 + Q19'
```

Run CFA model



```{r}
PALSfit2 <- cfa(latents.model2, data = PALS)#standard fit
summary(PALSfit2) #to view summary of the fit
partable(PALSfit2) # to have the summary of parameters converted to a dataframe
vartable(PALSfit2) # to have the summary of variables converted to a dataframe
parameterestimates(PALSfit2) #parameterEstimates() includes the estimates, standard errors, z value, p value, and 95% CIs for all model parameters.
```
To see a summary of the tests and esimtates of the model

```{r}
summary(PALSfit2,fit.measures = TRUE )
```
#Notes on 3-factor model

Comparative Fit Index (CFI)                    0.882
Tucker-Lewis Index (TLI)                       0.864
  
#Comparing the fits 
Here is a comparision between the 4-factor model (PALSfit1) and the 3-factor model (PALSfit2) 


```{r}
compareFit(PALSfit1, PALSfit2, nested=FALSE)
```

It appears that the CFI and TLI were higher in the 4 factor model, compared to the 3 factor model (0.920 vs. 0.882, and 0.907 vs. 0.864)

ALso the RMSEA increased from 0.067 to 0.081. Some people recommend 0.07 RMSEA. 

#Notes on the fit measures
chi-square needs to be significant 
CFI and TLI needs to be high, as they compare your model to a completely free model. Comparative Fit Index (CFI)         0.920
Tucker-Lewis Index (TLI)                       0.907 
RMSEA: needs to be low as possible  <0.06 is preferred


Variances: left over variance, that is not accounted for in the model. We want the variances to be insignificant. It is very unusual though to have non-sig vairances. 

#Residual matrix

```{r}
residuals(PALSfit)
```


#Calculating factor scores (refer to https://rdrr.io/cran/lavaan/man/lavPredict.html)

```{r}
fit <- cfa(latents.model, data = PALS, se = "robust", test = "Satorra-Bentler") 
head(lavPredict(fit)) #first we need to define the model
head(lavPredict(fit, type = "ov")) #OV stands for "observed variables". Then we use the lavpredict function to compute estimated values for latent variables.
idx <- lavInspect(fit, "case.idx") #The lavInspect() functions can be used to inspect/extract information that is stored inside (or can be computed from) a fitted lavaan object. 
PALSfscores <- lavPredict(fit)
## loop over factors
for(fs in colnames(fscores)) {
 PALS[idx, fs] <- fscores[ , fs]
}#I am not sure what does the last piece of code mean.
head(PALS)
```
#Note:I need to check with Michelle on how to interpret this output, and why do I have 6 scores for each question.

```{r}
summary(PALSfscores)
```

As you can see from the output, each case received a score for each factor. These are the scores to be used in the SEM later. 

Converting fscores from lavaan matrix to dataframe

```{r}
PALSfscores.df <- as.data.frame(PALSfscores)
```

#Matching the scores with the cases. To do so,I had to create row nos. in both cases to match them. 

Add new coloumn to existing df (idx) row no. to identify the cases
```{r}
PALSfscores.df$rowno = idx
```

Creating a new column for row no. in the PALS 

```{r}
PALS$rowno = seq.int(nrow(PALS))
```

```{r}
PALSfscore_joined <- inner_join (x = PALS, y= PALSfscores.df, by = "rowno")
PALSfscore_joined

```

when joining the two tables, 3 cases were missed!
```{r}
PALS_Factors <- select(PALSfscore_joined,ID:Gender, SelfEfficacy:MasteryGoals)
```

Saving PALSfacotrs to a new  file 
```{r}
write.csv(PALS_Factors, "PALS_Factors.csv")
```


#Checking normality of the fscores

```{r}
PALSF_hist <- ggplot(PALSfscores.df, aes(sum)) + 
geom_histogram(aes(y = ..density..), colour = "black", fill = "white") +
labs(x = "score", y = "")
```
#Adding normality curve

```{r}
PALS_hist + stat_function(fun = dnorm, args = list(mean = mean(PALS$sum,
na.rm = TRUE), sd = sd(PALS_Q$score, na.rm = TRUE)), colour = "black", size = 1)
```
Notes on 23/3:

I have removed those who didn't respond to the survey at all. I have also imputed the missing values for the rest. Now, my data set has 546 cases.After removing the missing values and calcuating the rest, the data looks very close to normaility,also when I ran the robust analysis using satora bentler the CFA output was even better. 

I recalculated the correlations, ran the CFA and calculated factor scores again. I will do the same with SALES. I have also saved the output in doc format using sink command. 



